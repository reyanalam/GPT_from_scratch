{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0837332-86bc-4839-beca-1f5ae3466f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d3d7fe-3832-47fd-8f27-6cd4bbb49400",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\Reyan\\Desktop\\LLM\\Tokenizer\\J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt\",'r') as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f34b72c-11f1-48e9-8a3e-e52f27131692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Harry Potter and the Sorcerer's Stone\\n\\n\\nCHAPTER ONE\\n\\nTHE BOY WHO LIVED\\n\\nMr. and Mrs. Dursley, of nu\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6ded0c1-4744-4f4b-83d6-36b90e944bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b1a76aa-2e3e-4729-8656-c7194f58885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size, max_length, stride, shuffle, drop_last, num_workers):\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbd0246e-8be5-4d55-bc74-d7ab42815df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  \n",
    "    \"context_length\": 256, \n",
    "    \"emb_dim\": 768,        \n",
    "    \"n_heads\": 12,         \n",
    "    \"n_layers\": 12,        \n",
    "    \"drop_rate\": 0.1,      \n",
    "    \"qkv_bias\": False     \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97c46f8f-189f-4a44-922a-59186b35789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "split_idx = int(len(raw_text)*train_ratio)\n",
    "train_data = raw_text[:split_idx]\n",
    "val_data = raw_text[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f2adae-8f2c-463d-8108-04f71e45e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader_v1(train_data,2,GPT_CONFIG_124M['context_length'],GPT_CONFIG_124M['context_length'],True,True,0)\n",
    "val_loader = create_dataloader_v1(val_data,2,GPT_CONFIG_124M['context_length'],GPT_CONFIG_124M['context_length'],False,False,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af92a4c4-eb09-4327-8d91-397e0a1250a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e895d5cf-cbba-4d56-abd2-e6fb56c6b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fdfbcbd-734f-46ea-bdfc-bdd363b0c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50413c0d-27b4-4a4c-be9e-a8106290a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feedforwardnn(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*cfg['emb_dim'],cfg['emb_dim'])\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1c765a0-6941-4aa0-a9a9-3a60f0add472",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiheadattentionv2(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_len,num_head):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.wq= torch.nn.Linear(d_in,d_out)\n",
    "        self.wk= torch.nn.Linear(d_in,d_out)\n",
    "        self.wv= torch.nn.Linear(d_in,d_out)\n",
    "\n",
    "        self.mask = torch.triu(torch.ones(context_len,context_len),diagonal=1)\n",
    "        self.num_head = num_head\n",
    "        self.head_dim = d_out//self.num_head\n",
    "        self.out_pro = torch.nn.Linear(d_out,d_out)\n",
    "\n",
    "    def forward(self,x):\n",
    "        b,num_token,d_in = x.shape\n",
    "        query = self.wq(x)\n",
    "        key = self.wk(x)\n",
    "        value = self.wv(x)\n",
    "\n",
    "        #d_out = num_head*head_dim\n",
    "        query = query.view(b,num_token,self.num_head,self.head_dim)\n",
    "        key = key.view(b,num_token,self.num_head,self.head_dim)\n",
    "        value = value.view(b,num_token,self.num_head,self.head_dim)\n",
    "\n",
    "        query = query.transpose(1,2)\n",
    "        key = key.transpose(1,2)\n",
    "        value = value.transpose(1,2)\n",
    "\n",
    "        att_score = query @ key.transpose(2,3)\n",
    "        masked_att_score = att_score.masked_fill(self.mask.bool()[:num_token, :num_token],-torch.inf)\n",
    "        masked_att_weight = torch.softmax(masked_att_score/(key.shape[-1]**0.5),dim=-1)\n",
    "\n",
    "        context_vec = (masked_att_weight @ value).transpose(1,2)\n",
    "        context_vec = context_vec.contiguous().view(b, num_token, self.d_out)\n",
    "        context_vec = self.out_pro(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea8375cd-68b0-4590-9c5f-378bfecc8fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.layernorm2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.ff = feedforwardnn(cfg)\n",
    "        self.mha = multiheadattentionv2(d_in=cfg['emb_dim'],d_out=cfg['emb_dim'],context_len=cfg['context_length'],num_head=cfg['n_heads'])\n",
    "\n",
    "    def forward(self,x):\n",
    "        shortcut=x\n",
    "        x=self.layernorm1(x)\n",
    "        x=self.mha(x)\n",
    "        x=x+shortcut\n",
    "        \n",
    "        shortcut=x\n",
    "        x=self.layernorm2(x)\n",
    "        x=self.ff(x)\n",
    "        x=x+shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb46df59-9f5c-49de-8d14-26cba25510af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.positional_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.trf = nn.Sequential(\n",
    "            *[Transformer(cfg) for _ in range(cfg['n_layers'])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"])\n",
    "        \n",
    "    def forward(self,input_idx):\n",
    "        b,seq_len = input_idx.shape\n",
    "        tok_embed = self.token_emb(input_idx)\n",
    "        pos_embed = self.positional_emb(torch.arange(seq_len))\n",
    "        x = tok_embed + pos_embed\n",
    "        x = self.dropout(x)\n",
    "        x = self.trf(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26813add-2654-4f8c-95b0-4d520c9809f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_emb): Embedding(50257, 768)\n",
       "  (positional_emb): Embedding(256, 768)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (trf): Sequential(\n",
       "    (0): Transformer(\n",
       "      (layernorm1): LayerNorm()\n",
       "      (layernorm2): LayerNorm()\n",
       "      (ff): feedforwardnn(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mha): multiheadattentionv2(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_pro): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Transformer(\n",
       "      (layernorm1): LayerNorm()\n",
       "      (layernorm2): LayerNorm()\n",
       "      (ff): feedforwardnn(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mha): multiheadattentionv2(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_pro): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Transformer(\n",
       "      (layernorm1): LayerNorm()\n",
       "      (layernorm2): LayerNorm()\n",
       "      (ff): feedforwardnn(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mha): multiheadattentionv2(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_pro): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Transformer(\n",
       "      (layernorm1): LayerNorm()\n",
       "      (layernorm2): LayerNorm()\n",
       "      (ff): feedforwardnn(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mha): multiheadattentionv2(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_pro): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Transformer(\n",
       "      (layernorm1): LayerNorm()\n",
       "      (layernorm2): LayerNorm()\n",
       "      (ff): feedforwardnn(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mha): multiheadattentionv2(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_pro): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Transformer(\n",
       "      (layernorm1): LayerNorm()\n",
       "      (layernorm2): LayerNorm()\n",
       "      (ff): feedforwardnn(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mha): multiheadattentionv2(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_pro): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Transformer(\n",
       "      (layernorm1): LayerNorm()\n",
       "      (layernorm2): LayerNorm()\n",
       "      (ff): feedforwardnn(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mha): multiheadattentionv2(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_pro): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Transformer(\n",
       "      (layernorm1): LayerNorm()\n",
       "      (layernorm2): LayerNorm()\n",
       "      (ff): feedforwardnn(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mha): multiheadattentionv2(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_pro): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (8): Transformer(\n",
       "      (layernorm1): LayerNorm()\n",
       "      (layernorm2): LayerNorm()\n",
       "      (ff): feedforwardnn(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mha): multiheadattentionv2(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_pro): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (9): Transformer(\n",
       "      (layernorm1): LayerNorm()\n",
       "      (layernorm2): LayerNorm()\n",
       "      (ff): feedforwardnn(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mha): multiheadattentionv2(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_pro): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (10): Transformer(\n",
       "      (layernorm1): LayerNorm()\n",
       "      (layernorm2): LayerNorm()\n",
       "      (ff): feedforwardnn(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mha): multiheadattentionv2(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_pro): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (11): Transformer(\n",
       "      (layernorm1): LayerNorm()\n",
       "      (layernorm2): LayerNorm()\n",
       "      (ff): feedforwardnn(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (mha): multiheadattentionv2(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_pro): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "210cd3ab-10ff-4fba-9e04-29365554ab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextwordprediction(model,idx,max_new_token,context_len):\n",
    "    for _ in range(max_new_token):\n",
    "        idx_cond = idx[:,-context_len:]\n",
    "        logits = model(idx_cond)\n",
    "        logit = logits[:,-1,:]\n",
    "        prob = torch.softmax(logit,dim=-1)\n",
    "        idx_next = torch.argmax(prob,dim=-1,keepdim=True)\n",
    "        idx = torch.cat((idx,idx_next),dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52f924d0-d3f2-4b1a-b5af-45756026c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss(input_batch,target_batch,model):\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0,1),target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def loss_function(data_loader,num_batch=None):\n",
    "    total_loss=0\n",
    "    if num_batch is None:\n",
    "        num_batch = len(data_loader)\n",
    "    num_batch = min(num_batch,len(data_loader))\n",
    "    for i,(input_batch,target_batch) in enumerate(data_loader):\n",
    "        if i < num_batch:\n",
    "            loss = cal_loss(input_batch,target_batch,GPT(GPT_CONFIG_124M))\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss/num_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c61b180b-bb6f-47e5-b170-43cf70d3fafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 10.995641010563547\n",
      "val_loss: 10.992317199707031\n"
     ]
    }
   ],
   "source": [
    "train_loss = loss_function(train_loader)\n",
    "val_loss = loss_function(val_loader)\n",
    "print('train_loss:',train_loss)\n",
    "print('val_loss:',val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69991707-ea07-40e2-b2bc-132528e34885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(num_epoch,train_loader,val_loader,optimizer,model,eval_freq,eval_iter,tokenizer):\n",
    "    train_losses=[]\n",
    "    val_losses=[]\n",
    "    global_step=-1\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        for input_batch,target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = cal_loss(input_batch,target_batch,model)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step+=1\n",
    "\n",
    "            if global_step%eval_freq ==0:\n",
    "                train_loss = loss_function(train_loader,num_batch=eval_iter)\n",
    "                val_loss = loss_function(val_loader,num_batch=eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "    return train_losses,val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a536590-5dee-441d-8253-d6719a4f53b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 11.031, Val loss 11.005\n",
      "Ep 1 (Step 000005): Train loss 10.975, Val loss 10.985\n",
      "Ep 1 (Step 000010): Train loss 10.969, Val loss 10.983\n",
      "Ep 1 (Step 000015): Train loss 10.984, Val loss 10.965\n",
      "Ep 1 (Step 000020): Train loss 10.989, Val loss 11.000\n",
      "Ep 1 (Step 000025): Train loss 10.985, Val loss 10.962\n",
      "Ep 1 (Step 000030): Train loss 10.979, Val loss 10.991\n",
      "Ep 1 (Step 000035): Train loss 11.011, Val loss 10.993\n",
      "Ep 1 (Step 000040): Train loss 10.988, Val loss 11.037\n",
      "Ep 1 (Step 000045): Train loss 10.983, Val loss 10.994\n",
      "Ep 1 (Step 000050): Train loss 11.003, Val loss 11.000\n",
      "Ep 1 (Step 000055): Train loss 10.974, Val loss 10.994\n",
      "Ep 1 (Step 000060): Train loss 11.000, Val loss 10.982\n",
      "Ep 1 (Step 000065): Train loss 10.995, Val loss 11.006\n",
      "Ep 1 (Step 000070): Train loss 10.988, Val loss 10.976\n",
      "Ep 1 (Step 000075): Train loss 10.971, Val loss 11.008\n",
      "Ep 1 (Step 000080): Train loss 10.954, Val loss 11.014\n",
      "Ep 1 (Step 000085): Train loss 11.009, Val loss 10.991\n",
      "Ep 1 (Step 000090): Train loss 10.992, Val loss 11.003\n",
      "Ep 1 (Step 000095): Train loss 10.997, Val loss 11.001\n",
      "Ep 1 (Step 000100): Train loss 10.994, Val loss 10.989\n",
      "Ep 1 (Step 000105): Train loss 10.974, Val loss 10.979\n",
      "Ep 1 (Step 000110): Train loss 10.981, Val loss 10.999\n",
      "Ep 1 (Step 000115): Train loss 10.988, Val loss 10.964\n",
      "Ep 1 (Step 000120): Train loss 11.009, Val loss 11.011\n",
      "Ep 1 (Step 000125): Train loss 10.973, Val loss 10.976\n",
      "Ep 1 (Step 000130): Train loss 10.990, Val loss 10.989\n",
      "Ep 1 (Step 000135): Train loss 11.000, Val loss 10.990\n",
      "Ep 1 (Step 000140): Train loss 10.986, Val loss 10.978\n",
      "Ep 1 (Step 000145): Train loss 11.015, Val loss 10.977\n",
      "Ep 1 (Step 000150): Train loss 11.029, Val loss 10.968\n",
      "Ep 1 (Step 000155): Train loss 11.001, Val loss 10.994\n",
      "Ep 1 (Step 000160): Train loss 10.970, Val loss 11.009\n",
      "Ep 1 (Step 000165): Train loss 11.008, Val loss 11.014\n",
      "Ep 1 (Step 000170): Train loss 11.037, Val loss 10.992\n",
      "Ep 1 (Step 000175): Train loss 10.963, Val loss 10.999\n",
      "Ep 1 (Step 000180): Train loss 11.002, Val loss 10.990\n",
      "Ep 1 (Step 000185): Train loss 11.006, Val loss 11.006\n",
      "Ep 1 (Step 000190): Train loss 10.966, Val loss 11.014\n",
      "Ep 1 (Step 000195): Train loss 10.968, Val loss 10.996\n",
      "Ep 1 (Step 000200): Train loss 10.995, Val loss 11.003\n",
      "Ep 2 (Step 000205): Train loss 10.986, Val loss 11.013\n",
      "Ep 2 (Step 000210): Train loss 10.992, Val loss 10.992\n",
      "Ep 2 (Step 000215): Train loss 10.988, Val loss 10.994\n",
      "Ep 2 (Step 000220): Train loss 10.990, Val loss 11.005\n",
      "Ep 2 (Step 000225): Train loss 11.014, Val loss 10.975\n",
      "Ep 2 (Step 000230): Train loss 10.974, Val loss 10.990\n",
      "Ep 2 (Step 000235): Train loss 10.985, Val loss 10.994\n",
      "Ep 2 (Step 000240): Train loss 10.974, Val loss 10.995\n",
      "Ep 2 (Step 000245): Train loss 10.996, Val loss 10.998\n",
      "Ep 2 (Step 000250): Train loss 10.980, Val loss 10.989\n",
      "Ep 2 (Step 000255): Train loss 10.992, Val loss 11.009\n",
      "Ep 2 (Step 000260): Train loss 11.008, Val loss 10.954\n",
      "Ep 2 (Step 000265): Train loss 11.012, Val loss 10.969\n",
      "Ep 2 (Step 000270): Train loss 10.989, Val loss 11.017\n",
      "Ep 2 (Step 000275): Train loss 11.003, Val loss 10.973\n",
      "Ep 2 (Step 000280): Train loss 10.983, Val loss 11.013\n",
      "Ep 2 (Step 000285): Train loss 10.993, Val loss 10.989\n",
      "Ep 2 (Step 000290): Train loss 10.986, Val loss 11.002\n",
      "Ep 2 (Step 000295): Train loss 10.983, Val loss 11.019\n",
      "Ep 2 (Step 000300): Train loss 11.007, Val loss 10.990\n",
      "Ep 2 (Step 000305): Train loss 10.984, Val loss 10.967\n",
      "Ep 2 (Step 000310): Train loss 11.013, Val loss 10.991\n",
      "Ep 2 (Step 000315): Train loss 10.991, Val loss 10.998\n",
      "Ep 2 (Step 000320): Train loss 11.027, Val loss 10.991\n",
      "Ep 2 (Step 000325): Train loss 10.979, Val loss 11.006\n",
      "Ep 2 (Step 000330): Train loss 11.000, Val loss 10.997\n",
      "Ep 2 (Step 000335): Train loss 11.012, Val loss 10.982\n",
      "Ep 2 (Step 000340): Train loss 10.991, Val loss 10.983\n",
      "Ep 2 (Step 000345): Train loss 11.000, Val loss 11.010\n",
      "Ep 2 (Step 000350): Train loss 10.971, Val loss 11.016\n",
      "Ep 2 (Step 000355): Train loss 11.016, Val loss 10.963\n",
      "Ep 2 (Step 000360): Train loss 10.977, Val loss 10.970\n",
      "Ep 2 (Step 000365): Train loss 10.981, Val loss 11.008\n",
      "Ep 2 (Step 000370): Train loss 10.995, Val loss 10.975\n",
      "Ep 2 (Step 000375): Train loss 10.998, Val loss 10.965\n",
      "Ep 2 (Step 000380): Train loss 10.984, Val loss 11.005\n",
      "Ep 2 (Step 000385): Train loss 10.986, Val loss 10.993\n",
      "Ep 2 (Step 000390): Train loss 10.984, Val loss 11.012\n",
      "Ep 2 (Step 000395): Train loss 10.992, Val loss 10.949\n",
      "Ep 2 (Step 000400): Train loss 10.994, Val loss 10.974\n",
      "Ep 2 (Step 000405): Train loss 10.999, Val loss 10.988\n",
      "Ep 3 (Step 000410): Train loss 11.019, Val loss 11.008\n",
      "Ep 3 (Step 000415): Train loss 10.999, Val loss 10.959\n",
      "Ep 3 (Step 000420): Train loss 10.999, Val loss 11.017\n",
      "Ep 3 (Step 000425): Train loss 10.978, Val loss 10.981\n",
      "Ep 3 (Step 000430): Train loss 10.993, Val loss 10.971\n",
      "Ep 3 (Step 000435): Train loss 10.997, Val loss 11.001\n",
      "Ep 3 (Step 000440): Train loss 11.000, Val loss 11.014\n",
      "Ep 3 (Step 000445): Train loss 11.004, Val loss 10.980\n",
      "Ep 3 (Step 000450): Train loss 10.977, Val loss 10.975\n",
      "Ep 3 (Step 000455): Train loss 10.999, Val loss 10.968\n",
      "Ep 3 (Step 000460): Train loss 10.992, Val loss 10.988\n",
      "Ep 3 (Step 000465): Train loss 10.988, Val loss 10.969\n",
      "Ep 3 (Step 000470): Train loss 10.998, Val loss 11.009\n",
      "Ep 3 (Step 000475): Train loss 10.981, Val loss 10.970\n",
      "Ep 3 (Step 000480): Train loss 10.986, Val loss 11.003\n",
      "Ep 3 (Step 000485): Train loss 10.988, Val loss 11.030\n",
      "Ep 3 (Step 000490): Train loss 11.017, Val loss 10.996\n",
      "Ep 3 (Step 000495): Train loss 10.989, Val loss 11.001\n",
      "Ep 3 (Step 000500): Train loss 11.003, Val loss 10.990\n",
      "Ep 3 (Step 000505): Train loss 10.978, Val loss 10.997\n",
      "Ep 3 (Step 000510): Train loss 10.982, Val loss 11.005\n",
      "Ep 3 (Step 000515): Train loss 11.017, Val loss 10.962\n",
      "Ep 3 (Step 000520): Train loss 11.000, Val loss 11.002\n",
      "Ep 3 (Step 000525): Train loss 11.013, Val loss 10.994\n",
      "Ep 3 (Step 000530): Train loss 11.009, Val loss 10.994\n",
      "Ep 3 (Step 000535): Train loss 10.986, Val loss 10.977\n",
      "Ep 3 (Step 000540): Train loss 10.984, Val loss 11.002\n",
      "Ep 3 (Step 000545): Train loss 10.994, Val loss 11.020\n",
      "Ep 3 (Step 000550): Train loss 10.971, Val loss 10.967\n",
      "Ep 3 (Step 000555): Train loss 10.995, Val loss 10.967\n",
      "Ep 3 (Step 000560): Train loss 10.996, Val loss 11.003\n",
      "Ep 3 (Step 000565): Train loss 10.977, Val loss 11.000\n",
      "Ep 3 (Step 000570): Train loss 10.980, Val loss 11.010\n",
      "Ep 3 (Step 000575): Train loss 10.977, Val loss 10.976\n",
      "Ep 3 (Step 000580): Train loss 10.989, Val loss 10.983\n",
      "Ep 3 (Step 000585): Train loss 10.987, Val loss 11.020\n",
      "Ep 3 (Step 000590): Train loss 10.963, Val loss 11.022\n",
      "Ep 3 (Step 000595): Train loss 10.984, Val loss 10.971\n",
      "Ep 3 (Step 000600): Train loss 11.018, Val loss 11.004\n",
      "Ep 3 (Step 000605): Train loss 10.993, Val loss 10.982\n",
      "Ep 3 (Step 000610): Train loss 10.966, Val loss 11.009\n",
      "Ep 4 (Step 000615): Train loss 11.000, Val loss 10.980\n",
      "Ep 4 (Step 000620): Train loss 10.999, Val loss 10.980\n",
      "Ep 4 (Step 000625): Train loss 10.971, Val loss 11.005\n",
      "Ep 4 (Step 000630): Train loss 10.990, Val loss 10.992\n",
      "Ep 4 (Step 000635): Train loss 11.006, Val loss 11.036\n",
      "Ep 4 (Step 000640): Train loss 10.990, Val loss 10.970\n",
      "Ep 4 (Step 000645): Train loss 10.991, Val loss 11.014\n",
      "Ep 4 (Step 000650): Train loss 10.986, Val loss 10.978\n",
      "Ep 4 (Step 000655): Train loss 10.964, Val loss 11.025\n",
      "Ep 4 (Step 000660): Train loss 10.980, Val loss 10.977\n",
      "Ep 4 (Step 000665): Train loss 10.980, Val loss 10.991\n",
      "Ep 4 (Step 000670): Train loss 10.980, Val loss 11.004\n",
      "Ep 4 (Step 000675): Train loss 11.009, Val loss 10.995\n",
      "Ep 4 (Step 000680): Train loss 10.987, Val loss 10.989\n",
      "Ep 4 (Step 000685): Train loss 10.987, Val loss 11.011\n",
      "Ep 4 (Step 000690): Train loss 11.017, Val loss 10.982\n",
      "Ep 4 (Step 000695): Train loss 10.987, Val loss 10.990\n",
      "Ep 4 (Step 000700): Train loss 10.986, Val loss 10.951\n",
      "Ep 4 (Step 000705): Train loss 11.018, Val loss 11.011\n",
      "Ep 4 (Step 000710): Train loss 10.986, Val loss 10.971\n",
      "Ep 4 (Step 000715): Train loss 10.997, Val loss 11.000\n",
      "Ep 4 (Step 000720): Train loss 10.990, Val loss 10.999\n",
      "Ep 4 (Step 000725): Train loss 11.005, Val loss 11.007\n",
      "Ep 4 (Step 000730): Train loss 10.973, Val loss 10.977\n",
      "Ep 4 (Step 000735): Train loss 10.981, Val loss 10.964\n",
      "Ep 4 (Step 000740): Train loss 11.008, Val loss 10.987\n",
      "Ep 4 (Step 000745): Train loss 10.984, Val loss 10.966\n",
      "Ep 4 (Step 000750): Train loss 11.000, Val loss 11.021\n",
      "Ep 4 (Step 000755): Train loss 10.970, Val loss 10.984\n",
      "Ep 4 (Step 000760): Train loss 10.981, Val loss 10.993\n",
      "Ep 4 (Step 000765): Train loss 11.002, Val loss 10.974\n",
      "Ep 4 (Step 000770): Train loss 10.995, Val loss 10.993\n",
      "Ep 4 (Step 000775): Train loss 10.993, Val loss 10.970\n",
      "Ep 4 (Step 000780): Train loss 10.981, Val loss 11.005\n",
      "Ep 4 (Step 000785): Train loss 10.981, Val loss 10.954\n",
      "Ep 4 (Step 000790): Train loss 10.993, Val loss 10.986\n",
      "Ep 4 (Step 000795): Train loss 10.999, Val loss 10.988\n",
      "Ep 4 (Step 000800): Train loss 11.021, Val loss 10.993\n",
      "Ep 4 (Step 000805): Train loss 10.991, Val loss 11.006\n",
      "Ep 4 (Step 000810): Train loss 10.983, Val loss 10.951\n",
      "Ep 4 (Step 000815): Train loss 11.004, Val loss 10.985\n",
      "Ep 5 (Step 000820): Train loss 10.993, Val loss 10.987\n",
      "Ep 5 (Step 000825): Train loss 10.990, Val loss 11.000\n",
      "Ep 5 (Step 000830): Train loss 11.000, Val loss 10.973\n",
      "Ep 5 (Step 000835): Train loss 10.986, Val loss 11.006\n",
      "Ep 5 (Step 000840): Train loss 10.999, Val loss 11.003\n",
      "Ep 5 (Step 000845): Train loss 10.992, Val loss 10.974\n",
      "Ep 5 (Step 000850): Train loss 10.984, Val loss 11.014\n",
      "Ep 5 (Step 000855): Train loss 10.972, Val loss 11.009\n",
      "Ep 5 (Step 000860): Train loss 10.987, Val loss 11.004\n",
      "Ep 5 (Step 000865): Train loss 10.983, Val loss 10.985\n",
      "Ep 5 (Step 000870): Train loss 10.989, Val loss 11.003\n",
      "Ep 5 (Step 000875): Train loss 10.981, Val loss 10.963\n",
      "Ep 5 (Step 000880): Train loss 10.995, Val loss 10.995\n",
      "Ep 5 (Step 000885): Train loss 11.012, Val loss 11.001\n",
      "Ep 5 (Step 000890): Train loss 10.991, Val loss 10.991\n",
      "Ep 5 (Step 000895): Train loss 10.988, Val loss 10.994\n",
      "Ep 5 (Step 000900): Train loss 10.990, Val loss 10.983\n",
      "Ep 5 (Step 000905): Train loss 10.991, Val loss 10.979\n",
      "Ep 5 (Step 000910): Train loss 11.022, Val loss 10.981\n",
      "Ep 5 (Step 000915): Train loss 11.006, Val loss 10.983\n",
      "Ep 5 (Step 000920): Train loss 10.986, Val loss 10.979\n",
      "Ep 5 (Step 000925): Train loss 10.973, Val loss 10.984\n",
      "Ep 5 (Step 000930): Train loss 11.002, Val loss 10.994\n",
      "Ep 5 (Step 000935): Train loss 10.983, Val loss 11.014\n",
      "Ep 5 (Step 000940): Train loss 10.982, Val loss 10.978\n",
      "Ep 5 (Step 000945): Train loss 11.005, Val loss 11.005\n",
      "Ep 5 (Step 000950): Train loss 10.981, Val loss 10.983\n",
      "Ep 5 (Step 000955): Train loss 10.975, Val loss 10.967\n",
      "Ep 5 (Step 000960): Train loss 10.990, Val loss 11.009\n",
      "Ep 5 (Step 000965): Train loss 11.005, Val loss 10.985\n",
      "Ep 5 (Step 000970): Train loss 10.989, Val loss 10.999\n",
      "Ep 5 (Step 000975): Train loss 10.975, Val loss 10.980\n",
      "Ep 5 (Step 000980): Train loss 10.993, Val loss 11.002\n",
      "Ep 5 (Step 000985): Train loss 10.992, Val loss 10.958\n",
      "Ep 5 (Step 000990): Train loss 10.993, Val loss 10.989\n",
      "Ep 5 (Step 000995): Train loss 10.999, Val loss 10.995\n",
      "Ep 5 (Step 001000): Train loss 10.988, Val loss 11.027\n",
      "Ep 5 (Step 001005): Train loss 10.997, Val loss 10.989\n",
      "Ep 5 (Step 001010): Train loss 11.027, Val loss 10.959\n",
      "Ep 5 (Step 001015): Train loss 11.007, Val loss 10.985\n",
      "Ep 5 (Step 001020): Train loss 10.996, Val loss 10.979\n",
      "Ep 6 (Step 001025): Train loss 10.990, Val loss 10.961\n",
      "Ep 6 (Step 001030): Train loss 10.982, Val loss 10.994\n",
      "Ep 6 (Step 001035): Train loss 11.013, Val loss 11.006\n",
      "Ep 6 (Step 001040): Train loss 10.984, Val loss 10.965\n",
      "Ep 6 (Step 001045): Train loss 11.002, Val loss 10.997\n",
      "Ep 6 (Step 001050): Train loss 10.989, Val loss 10.999\n",
      "Ep 6 (Step 001055): Train loss 10.985, Val loss 10.967\n",
      "Ep 6 (Step 001060): Train loss 10.989, Val loss 10.973\n",
      "Ep 6 (Step 001065): Train loss 10.989, Val loss 10.980\n",
      "Ep 6 (Step 001070): Train loss 10.988, Val loss 10.998\n",
      "Ep 6 (Step 001075): Train loss 10.990, Val loss 11.011\n",
      "Ep 6 (Step 001080): Train loss 10.991, Val loss 10.982\n",
      "Ep 6 (Step 001085): Train loss 10.965, Val loss 10.992\n",
      "Ep 6 (Step 001090): Train loss 11.014, Val loss 10.985\n",
      "Ep 6 (Step 001095): Train loss 11.010, Val loss 10.951\n",
      "Ep 6 (Step 001100): Train loss 10.985, Val loss 10.987\n",
      "Ep 6 (Step 001105): Train loss 11.022, Val loss 11.003\n",
      "Ep 6 (Step 001110): Train loss 10.961, Val loss 10.974\n",
      "Ep 6 (Step 001115): Train loss 10.974, Val loss 10.990\n",
      "Ep 6 (Step 001120): Train loss 10.969, Val loss 10.995\n",
      "Ep 6 (Step 001125): Train loss 10.982, Val loss 10.990\n",
      "Ep 6 (Step 001130): Train loss 10.991, Val loss 11.000\n",
      "Ep 6 (Step 001135): Train loss 11.011, Val loss 10.965\n",
      "Ep 6 (Step 001140): Train loss 11.003, Val loss 10.993\n",
      "Ep 6 (Step 001145): Train loss 10.999, Val loss 10.984\n",
      "Ep 6 (Step 001150): Train loss 10.988, Val loss 10.961\n",
      "Ep 6 (Step 001155): Train loss 10.999, Val loss 11.002\n",
      "Ep 6 (Step 001160): Train loss 10.981, Val loss 11.025\n",
      "Ep 6 (Step 001165): Train loss 10.979, Val loss 11.010\n",
      "Ep 6 (Step 001170): Train loss 10.993, Val loss 10.999\n",
      "Ep 6 (Step 001175): Train loss 10.983, Val loss 10.998\n",
      "Ep 6 (Step 001180): Train loss 10.994, Val loss 10.988\n",
      "Ep 6 (Step 001185): Train loss 10.998, Val loss 10.977\n",
      "Ep 6 (Step 001190): Train loss 10.981, Val loss 10.997\n",
      "Ep 6 (Step 001195): Train loss 10.997, Val loss 10.976\n",
      "Ep 6 (Step 001200): Train loss 10.990, Val loss 10.995\n",
      "Ep 6 (Step 001205): Train loss 11.004, Val loss 11.000\n",
      "Ep 6 (Step 001210): Train loss 11.011, Val loss 10.974\n",
      "Ep 6 (Step 001215): Train loss 10.986, Val loss 10.990\n",
      "Ep 6 (Step 001220): Train loss 11.008, Val loss 10.978\n",
      "Ep 6 (Step 001225): Train loss 11.009, Val loss 11.002\n",
      "Ep 7 (Step 001230): Train loss 10.998, Val loss 10.974\n",
      "Ep 7 (Step 001235): Train loss 10.973, Val loss 11.006\n",
      "Ep 7 (Step 001240): Train loss 10.988, Val loss 10.993\n",
      "Ep 7 (Step 001245): Train loss 11.005, Val loss 10.991\n",
      "Ep 7 (Step 001250): Train loss 10.988, Val loss 10.962\n",
      "Ep 7 (Step 001255): Train loss 10.991, Val loss 11.009\n",
      "Ep 7 (Step 001260): Train loss 11.005, Val loss 10.999\n",
      "Ep 7 (Step 001265): Train loss 10.977, Val loss 10.986\n",
      "Ep 7 (Step 001270): Train loss 11.007, Val loss 10.977\n",
      "Ep 7 (Step 001275): Train loss 10.970, Val loss 10.985\n",
      "Ep 7 (Step 001280): Train loss 10.989, Val loss 10.994\n",
      "Ep 7 (Step 001285): Train loss 11.009, Val loss 11.001\n",
      "Ep 7 (Step 001290): Train loss 11.001, Val loss 10.993\n",
      "Ep 7 (Step 001295): Train loss 11.009, Val loss 10.966\n",
      "Ep 7 (Step 001300): Train loss 10.991, Val loss 11.008\n",
      "Ep 7 (Step 001305): Train loss 10.981, Val loss 10.970\n",
      "Ep 7 (Step 001310): Train loss 10.976, Val loss 10.975\n",
      "Ep 7 (Step 001315): Train loss 11.009, Val loss 10.993\n",
      "Ep 7 (Step 001320): Train loss 10.987, Val loss 11.000\n"
     ]
    }
   ],
   "source": [
    "model = GPT(GPT_CONFIG_124M)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "num_epochs = 10\n",
    "train_losses, val_losses = training(num_epochs,train_loader, val_loader, optimizer, model, eval_freq=5, eval_iter=5,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629aab8c-c174-4265-b3e0-11624201368a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
